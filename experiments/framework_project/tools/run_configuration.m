function [best_arms, regret] = run_configuration(thresholds, arms, values, policy, n_iterations, n_fix, opt_policies)
%MAX_KLDIV maximizes the kl divergense under a constraint that KL(p,x) \leq lim
%   Input:
%       thresholds  : thresholds generated by GENERATE_THRESHOLDS
%           % if threshold is sparse -> used as answer
%           % if threshold not sparse -> used as threshold
%       arms        : arms for the considered configuration 
%       values      : true values of the arms of the considered configuration
%       policy      : policy to consider in the MAB problem
%       n_iterations: number of time instant where we run the policy
%       n_fix       : number of steps for which we sample a single arm
%       opt_policies: option for the chosen policy generated by
%           GENERATE_OPT_POLICIES
%
%   Output:
%       best_arms: arms considered at each time instant by the considered
%           policy
%       regret   : cumulated regret for the considered policy
%

%   Copyright 2015 Paladino, S. and Trovo', F., Politecnico di Milano

best_arms = zeros(n_iterations,1);
regret = zeros(n_iterations+1,1);
[~,optimal_arm] = max(values);

if size(arms,1) > size(arms,2)
    arms = arms';
end

p0 = zeros(size(arms));
p1 = zeros(size(arms));

% Policies' options
opt_policies.ucb2.epochs = zeros(size(arms)); % number of epochs played by each arm used by ucb2


% Cycle: one iteration per arm
for t = 1 : numel(arms)
    
    % Arm selection
    idx_best_arm = t;
    best_arms(t) = arms(idx_best_arm);
    
    % Simulate user
    if issparse(thresholds)
        p1(idx_best_arm) = p1(idx_best_arm) + sum( thresholds(idx_best_arm,t) );
        p0(idx_best_arm) = p0(idx_best_arm) + sum( ~thresholds(idx_best_arm,t) );        
    else
        p1(idx_best_arm) = p1(idx_best_arm) + sum( arms(idx_best_arm) <= thresholds(t) );
        p0(idx_best_arm) = p0(idx_best_arm) + sum( ~(arms(idx_best_arm) <= thresholds(t)) );
    end
    
    % Update regret
    regret(t+1) = regret(t) + values(optimal_arm) - values(idx_best_arm);
end

t = t + 1;

% Main cycle
while t <= n_iterations
    
    % Arm selection
    [idx_best_arm, opt_policies] = policy(arms, p0, p1, t, opt_policies); 

    % n_turns to play according if UCB2
    n_turns = opt_policies.ucb2.n_turns * (opt_policies.ucb2.n_turns > n_fix) + n_fix * (opt_policies.ucb2.n_turns <= n_fix);
    
    % Inner cycle
    if t+n_turns > n_iterations
        n_turns = n_turns - ((t+n_turns) - n_iterations) + 1; % resize n_turns to fit n_iterations
    end
    
    best_arms(t:t+n_turns-1) = arms(idx_best_arm);
    if issparse(thresholds)
        p1(idx_best_arm) = p1(idx_best_arm) + sum( thresholds(idx_best_arm,t:t+n_turns-1) );
        p0(idx_best_arm) = p0(idx_best_arm) + sum( ~thresholds(idx_best_arm,t:t+n_turns-1) );
    else
        p1(idx_best_arm) = p1(idx_best_arm) + sum( arms(idx_best_arm) <= thresholds(t:t+n_turns-1) );
        p0(idx_best_arm) = p0(idx_best_arm) + sum( ~(arms(idx_best_arm) <= thresholds(t:t+n_turns-1)) );
    end
    
    regret(t+1:t+n_turns) = regret(t) + (values(optimal_arm) - values(idx_best_arm)) * (1:n_turns);
    
    t = t + n_turns;
    
end

regret = regret(2:end);
